import torch
from PIL import Image
from transformers import ChineseCLIPProcessor, ChineseCLIPModel
import torch.nn.functional as F


class EmbeddingService:
    def __init__(self):
        print("ğŸ”„ Loading Chinese-CLIP model...")
        model_name = "OFA-Sys/chinese-clip-vit-base-patch16"

        # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡ (M1/M2 ä½¿ç”¨ mpsï¼ŒNvidia ä½¿ç”¨ cudaï¼Œå¦åˆ™ cpu)
        if torch.cuda.is_available():
            self.device = "cuda"
        elif torch.backends.mps.is_available():
            self.device = "mps"
        else:
            self.device = "cpu"

        print(f"ğŸš€ Using device: {self.device}")

        self.model = ChineseCLIPModel.from_pretrained(model_name).to(self.device)
        self.processor = ChineseCLIPProcessor.from_pretrained(model_name)
        self.model.eval()  # å¼€å¯è¯„ä¼°æ¨¡å¼
        print("âœ… Chinese-CLIP loaded.")

    @torch.no_grad()  # ä¸è®¡ç®—æ¢¯åº¦ï¼Œçœå†…å­˜
    def embed_text(self, text: str):
        inputs = self.processor(text=[text], return_tensors="pt", padding=True).to(self.device)
        # è®¡ç®—ç‰¹å¾
        features = self.model.get_text_features(**inputs)
        # å½’ä¸€åŒ– (å…³é”®ï¼Elasticsearch Cosine Similarity éœ€è¦å½’ä¸€åŒ–å‘é‡)
        features = F.normalize(features, p=2, dim=1)
        # è½¬å› CPU å¹¶è½¬ä¸º List
        return features.cpu().numpy()[0].tolist()

    @torch.no_grad()
    def embed_image(self, image: Image.Image):
        inputs = self.processor(images=image, return_tensors="pt").to(self.device)
        features = self.model.get_image_features(**inputs)
        features = F.normalize(features, p=2, dim=1)
        return features.cpu().numpy()[0].tolist()


# å•ä¾‹æ¨¡å¼
embedding_service = EmbeddingService()