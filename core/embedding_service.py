import torch
from transformers import AltCLIPModel, AltCLIPProcessor
import torch.nn.functional as F
import os


class EmbeddingService:
    def __init__(self):
        print("ğŸ”„ Loading AltCLIP model (BAAI)...")
        # 1. è®¾ç½®å›½å†…é•œåƒï¼Œé˜²æ­¢ä¸‹è½½å¡æ­»
        os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

        # 2. æŒ‡å®šæ¨¡å‹ ID
        self.model_name = "BAAI/AltCLIP"

        # 3. å¼ºåˆ¶ä½¿ç”¨ CPU (M1 ä¸Šæœ€ç¨³å¦¥ã€æœ€å¿«çš„æ–¹å¼ï¼Œä¸”æ— å…¼å®¹æ€§é—®é¢˜)
        self.device = "cpu"
        print(f"ğŸš€ Embedding Service using device: {self.device}")

        # 4. åŠ è½½æ¨¡å‹
        # AltCLIP æ˜¯æ ‡å‡†æ¶æ„ï¼Œtransformers æ”¯æŒæå¥½
        self.model = AltCLIPModel.from_pretrained(self.model_name).to(self.device)
        self.processor = AltCLIPProcessor.from_pretrained(self.model_name)
        self.model.eval()
        print("âœ… AltCLIP loaded.")

    @torch.no_grad()
    def embed_text(self, text: str):
        # 1. é¢„å¤„ç†æ–‡æœ¬
        # padding=True, truncation=True æ˜¯æ ‡å‡†å†™æ³•
        inputs = self.processor(
            text=[text],
            padding=True,
            truncation=True,
            max_length=77,
            return_tensors="pt"
        ).to(self.device)

        # 2. è·å–æ–‡æœ¬ç‰¹å¾
        features = self.model.get_text_features(**inputs)

        # 3. å½’ä¸€åŒ– (Elasticsearch Cosine å¿…éœ€)
        features = F.normalize(features, p=2, dim=1)

        # 4. è½¬åˆ—è¡¨
        return features.cpu().numpy()[0].tolist()

    @torch.no_grad()
    def embed_image(self, image):
        # 1. é¢„å¤„ç†å›¾ç‰‡
        inputs = self.processor(
            images=image,
            return_tensors="pt"
        ).to(self.device)

        # 2. è·å–å›¾ç‰‡ç‰¹å¾
        features = self.model.get_image_features(**inputs)

        # 3. å½’ä¸€åŒ–
        features = F.normalize(features, p=2, dim=1)

        return features.cpu().numpy()[0].tolist()


# å•ä¾‹å¯¼å‡º
embedding_service = EmbeddingService()